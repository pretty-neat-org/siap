{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFG7MLOC0iA4kjsrfp8bQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pretty-neat-org/siap/blob/main/GloVe_good_data_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wINKF-mbrj1U"
      },
      "source": [
        "## GloVe embeddings + classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_4XPLFlg2Ur",
        "outputId": "afb3a9dc-adf7-4e8d-e4df-16a9fe76e674"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive/SIAP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/mydrive/My Drive': File exists\n",
            "config.json\t   file.csv\t\t\tin_domain_train.tsv\n",
            "data.csv\t   finalized_model_10k_SVE.sav\tpytorch_model.bin\n",
            "data_mini_10k.csv  finalized_model.sav\t\tserialized\n",
            "file5m.zip\t   glove.6B.300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw1o8K6YgrUk",
        "outputId": "338a4787-77c6-4ece-a095-ca586300064c"
      },
      "source": [
        "import string\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# TODO: Dotrenirati glove\n",
        "\n",
        "not_found = []\n",
        "\n",
        "\n",
        "class GloveVectorizer:\n",
        "    def __init__(self):\n",
        "        # load in pre-trained word vectors\n",
        "        print('Loading word vectors...')\n",
        "        word2vec = {}\n",
        "        embedding = []\n",
        "        idx2word = []\n",
        "        with open('gdrive/MyDrive/SIAP/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
        "            # is just a space-separated text file in the format:\n",
        "            # word vec[0] vec[1] vec[2] ...\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vec = np.asarray(values[1:], dtype='float32')\n",
        "                word2vec[word] = vec\n",
        "                embedding.append(vec)\n",
        "                idx2word.append(word)\n",
        "        print('Found %s word vectors.' % len(word2vec))\n",
        "\n",
        "        # save for later\n",
        "        self.word2vec = word2vec\n",
        "        self.embedding = np.array(embedding)\n",
        "        self.word2idx = {v: k for k, v in enumerate(idx2word)}\n",
        "        self.V, self.D = self.embedding.shape\n",
        "\n",
        "    def fit(self, data):\n",
        "        pass\n",
        "\n",
        "    def transform(self, data):\n",
        "        X = np.zeros((len(data), self.D))\n",
        "        n = 0\n",
        "        emptycount = 0\n",
        "        for sentence in data:\n",
        "            try:\n",
        "                tokens = sentence.lower().split()\n",
        "            except:\n",
        "                pass\n",
        "            vecs = []\n",
        "            for word in tokens:\n",
        "                if word in self.word2vec:\n",
        "                    vec = self.word2vec[word]\n",
        "                    vecs.append(vec)\n",
        "                else:\n",
        "                    not_found.append(word)\n",
        "\n",
        "            if len(vecs) > 0:\n",
        "                vecs = np.array(vecs)\n",
        "                X[n] = vecs.mean(axis=0)\n",
        "            else:\n",
        "                emptycount += 1\n",
        "            n += 1\n",
        "        print(\"Number of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "        return X\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        self.fit(data)\n",
        "        return self.transform(data)\n",
        "\n",
        "\n",
        "df = pd.read_csv('gdrive/MyDrive/SIAP/file.csv')\n",
        "print(df.columns)\n",
        "\n",
        "df['index'] = df.index\n",
        "\n",
        "print(df['controversiality'].value_counts())\n",
        "\n",
        "df_majority = df[df.controversiality == 0]\n",
        "df_minority = df[df.controversiality == 1]\n",
        "\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,\n",
        "                                   # sample without replacement # stavio sam tretno na true, da bi nastavio dalje\n",
        "                                   n_samples=10744,  # to match minority class\n",
        "                                   random_state=123)\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_minority, df_majority_downsampled])\n",
        "\n",
        "# Display new class counts\n",
        "print(df_upsampled.controversiality.value_counts())\n",
        "\n",
        "df = df_upsampled\n",
        "\n",
        "# ukloni linkove punctiation i kineski\n",
        "df['body'] = df['body'].str.replace(r'http\\S+', '')\n",
        "df['body'] = df['body'].str.replace(r'[^\\w\\s]', '')\n",
        "df['body'] = df['body'].str.replace(r\"([^\\x00-\\x7F])+\", '')\n",
        "# # remove numbers\n",
        "# df['body'] = df['body'].str.replace('\\d+', '')\n",
        "# df['body'] = df['body'].str.replace('_', '')\n",
        "# df['body'] = df['body'].str.replace('\\s+', '')\n",
        "\n",
        "\n",
        "sentences = df.body.values\n",
        "labels = df.controversiality.values\n",
        "\n",
        "train_inputs, train_val_inputs, train_labels, train_val_labels = train_test_split(sentences, labels,\n",
        "                                                                                    random_state=2018, test_size=0.3)\n",
        "test_inputs, validation_inputs, test_labels, validation_labels = train_test_split(train_val_inputs, train_val_labels,\n",
        "                                                                                    random_state=2018, test_size=0.5)\n",
        "\n",
        "vectorizer = GloveVectorizer()\n",
        "\n",
        "Xtrain = vectorizer.fit_transform(train_inputs)\n",
        "Ytrain = train_labels\n",
        "\n",
        "Xtest = vectorizer.transform(validation_inputs)\n",
        "Ytest = validation_labels\n",
        "\n",
        "transformer = Normalizer().fit(Xtrain)\n",
        "Xtrain = transformer.transform(Xtrain)\n",
        "Xtest = transformer.transform(Xtest)\n",
        "\n",
        "print(Counter(not_found).most_common(50))\n",
        "\n",
        "clfs = {\n",
        "    #'mnb': MultinomialNB(),\n",
        "    'gnb': GaussianNB(),\n",
        "    'svm1': SVC(kernel='linear'),\n",
        "    'svm2': SVC(kernel='rbf'),\n",
        "    'svm3': SVC(kernel='sigmoid'),\n",
        "    'mlp1': MLPClassifier(),\n",
        "    'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n",
        "    'ada': AdaBoostClassifier(),\n",
        "    'dtc': DecisionTreeClassifier(),\n",
        "    'rfc': RandomForestClassifier(),\n",
        "    'gbc': GradientBoostingClassifier(),\n",
        "    'lr': LogisticRegression()\n",
        "}\n",
        "\n",
        "f1_scores = dict()\n",
        "for clf_name in clfs:\n",
        "    print(clf_name)\n",
        "    clf = clfs[clf_name]\n",
        "    clf.fit(Xtrain, Ytrain)\n",
        "    y_pred = clf.predict(Xtest)\n",
        "    f1_scores[clf_name] = f1_score(y_pred, Ytest)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['created_utc', 'ups', 'subreddit_id', 'link_id', 'name', 'score_hidden',\n",
            "       'author_flair_css_class', 'author_flair_text', 'subreddit', 'id',\n",
            "       'removal_reason', 'gilded', 'downs', 'archived', 'author', 'score',\n",
            "       'retrieved_on', 'body', 'distinguished', 'edited', 'controversiality',\n",
            "       'parent_id'],\n",
            "      dtype='object')\n",
            "0    489256\n",
            "1     10744\n",
            "Name: controversiality, dtype: int64\n",
            "1    10744\n",
            "0    10744\n",
            "Name: controversiality, dtype: int64\n",
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n",
            "Number of samples with no words found: 285 / 15041\n",
            "Number of samples with no words found: 49 / 3224\n",
            "[('youve', 126), ('shouldnt', 111), ('downvoted', 79), ('subreddit', 77), ('werent', 71), ('theyve', 62), ('downvote', 59), ('theyll', 54), ('downvotes', 51), ('itll', 46), ('lmao', 43), ('upvote', 39), ('wouldve', 37), ('idk', 32), ('downvoting', 31), ('sjws', 28), ('theyd', 28), ('upvoted', 27), ('botrautomoderatorcommentsq11puwhat_is_automoderator', 25), ('gtthe', 24), ('hahaha', 21), ('lt3', 21), ('skyrim', 20), ('hadnt', 20), ('shouldve', 20), ('upvotes', 19), ('circlejerk', 19), ('couldve', 19), ('butthurt', 18), ('shitting', 18), ('redditors', 17), ('tbh', 16), ('cp3', 15), ('tldr', 15), ('amp009', 15), ('shits', 14), ('thatll', 13), ('ayy', 13), ('everyones', 13), ('dner', 13), ('dumbass', 12), ('cmon', 11), ('reddits', 11), ('amiibo', 11), ('nsfw', 11), ('amirite', 11), ('gtif', 11), ('inb4', 11), ('sharingan', 11), ('rekt', 10)]\n",
            "gnb\n",
            "svm1\n",
            "svm2\n",
            "svm3\n",
            "mlp1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mlp2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ada\n",
            "dtc\n",
            "rfc\n",
            "gbc\n",
            "lr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3La_KVCJMotQ",
        "outputId": "79e39cd5-0691-4917-bc25-4cb8879d942b"
      },
      "source": [
        "print(f1_scores)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gnb': 0.616871245756072, 'svm1': 0.6326224091318715, 'svm2': 0.6346389228886169, 'svm3': 0.5202520252025202, 'mlp1': 0.5677335919818947, 'mlp2': 0.5816112608565439, 'ada': 0.6054216867469879, 'dtc': 0.5551819015591563, 'rfc': 0.5978987583572111, 'gbc': 0.6268115942028986, 'lr': 0.6365539934190846}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvfVQjU5gNt7"
      },
      "source": [
        "Grid search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sB3hMn5fiji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a67a1b-8d5c-40e0-ffa2-e7079338947b"
      },
      "source": [
        "# defining parameter range\n",
        "param_grid = {'C': [0.1, 1, 10, 100], \n",
        "              'gamma': [1, 0.1],\n",
        "              'kernel': ['rbf']} \n",
        "  \n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "grid.fit(Xtrain, Ytrain)\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.630, total= 1.3min\n",
            "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.631, total= 1.3min\n",
            "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.619, total= 1.3min\n",
            "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.636, total= 1.3min\n",
            "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.617, total= 1.3min\n",
            "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.539, total= 1.4min\n",
            "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.553, total= 1.3min\n",
            "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.546, total= 1.3min\n",
            "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.533, total= 1.3min\n",
            "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.557, total= 1.3min\n",
            "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
            "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.643, total= 1.2min\n",
            "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
            "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.643, total= 1.2min\n",
            "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
            "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.628, total= 1.1min\n",
            "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
            "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.644, total= 1.2min\n",
            "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
            "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.629, total= 1.2min\n",
            "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
            "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.634, total= 1.2min\n",
            "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
            "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.621, total= 1.2min\n",
            "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
            "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.621, total= 1.2min\n",
            "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
            "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.636, total= 1.2min\n",
            "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
            "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.621, total= 1.2min\n",
            "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
            "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.636, total= 1.4min\n",
            "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
            "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.633, total= 1.4min\n",
            "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
            "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.627, total= 1.4min\n",
            "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
            "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.628, total= 1.4min\n",
            "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
            "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.616, total= 1.4min\n",
            "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
            "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.642, total= 1.2min\n",
            "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
            "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.637, total= 1.1min\n",
            "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
            "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.622, total= 1.1min\n",
            "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
            "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.639, total= 1.1min\n",
            "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
            "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.627, total= 1.1min\n",
            "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.608, total= 3.4min\n",
            "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.602, total= 3.4min\n",
            "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.614, total= 3.4min\n",
            "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.597, total= 3.3min\n",
            "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
            "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.603, total= 3.4min\n",
            "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.641, total= 1.4min\n",
            "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.637, total= 1.4min\n",
            "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.629, total= 1.4min\n",
            "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.641, total= 1.4min\n",
            "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
            "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.626, total= 1.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 61.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiHzdKBVkDQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9621269c-5149-4eee-afb8-84694a96f977"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)\n",
        "grid_predictions = grid.predict(Xtest)\n",
        "  \n",
        "# print classification report\n",
        "print(classification_report(Ytest, grid_predictions))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
            "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.62      0.63      1601\n",
            "           1       0.63      0.64      0.64      1623\n",
            "\n",
            "    accuracy                           0.63      3224\n",
            "   macro avg       0.63      0.63      0.63      3224\n",
            "weighted avg       0.63      0.63      0.63      3224\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08MN6TYLa-Zr",
        "outputId": "4bd4fefa-14bc-4303-bc1a-3a748d08fdd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Xtest = vectorizer.transform(test_inputs)\n",
        "Ytest = test_labels\n",
        "\n",
        "Xtest = transformer.transform(Xtest)\n",
        "grid_predictions = grid.predict(Xtest)\n",
        "  \n",
        "# print classification report\n",
        "print(classification_report(Ytest, grid_predictions))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples with no words found: 48 / 3223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.63      0.65      1670\n",
            "           1       0.62      0.65      0.63      1553\n",
            "\n",
            "    accuracy                           0.64      3223\n",
            "   macro avg       0.64      0.64      0.64      3223\n",
            "weighted avg       0.64      0.64      0.64      3223\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}